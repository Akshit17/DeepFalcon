{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MAF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ka8lyEjede2j",
        "EDhswINvqMKO",
        "rUSjnEc7qaVP",
        "LgQvEPcVkKuc",
        "rV4VYtODrhqG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lfxMlRhyUwY"
      },
      "source": [
        "# METADATA\n",
        "- Notebook implementing MAF - Masked Autoregressive Flow for Density Estimation. \n",
        "    - Only consider X and Y, ignoring particle energy for now.\n",
        "    - Scaling the X and Y predictions to [0, 125].\n",
        "    - Considering only the jets with 'valid' number of hits, i.e. in the range [384, 768].\n",
        "    - Binning the energies between [50, 55].\n",
        "- Author: Aditya Ahuja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAf4K32CIy"
      },
      "source": [
        "# Add a description for the Weights and Bisases project. \n",
        "WANDB_DESC = 'Setting up MAF.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5s5FAYwynHc"
      },
      "source": [
        "# PRELIMINARY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIw1sUgkIUt5"
      },
      "source": [
        "- We need PyTorch 1.9.0+cu102.\n",
        "- Ideally: \n",
        "\n",
        "```\n",
        ">>> pip list | grep torch\n",
        "torch                         1.9.0+cu102\n",
        "torchsummary                  1.5.1\n",
        "torchtext                     0.10.0\n",
        "torchvision                   0.10.0+cu102\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHloW2PINLm"
      },
      "source": [
        "%%capture\n",
        "! pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9wbr2sCINJa",
        "outputId": "fc669f15-1f7e-4de9-c395-86de4764933d"
      },
      "source": [
        "import torch\n",
        "assert torch.__version__ == '1.9.0+cu102'\n",
        "! pip list | grep torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.9.0+cu102\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.10.0\n",
            "torchvision                   0.10.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P6iwoPRun93",
        "outputId": "e7cc9118-13bc-463e-db4b-e7a25e9f6b79"
      },
      "source": [
        "# Log into Weights and Biases for Logging\n",
        "! wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madiah80\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DIfkHDiIiF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkKVumsIjTI"
      },
      "source": [
        "## Preliminary Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vc4EhGGux5R",
        "outputId": "b03840f2-c728-457d-82f9-44f3e8433f05"
      },
      "source": [
        "''' COLAB'''\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install missing packages\n",
        "! apt-get install tree >/dev/null\n",
        "\n",
        "# Download dataset\n",
        "# ! ./get_dataset.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yA1ipjvu8RJ"
      },
      "source": [
        "# Set Input format of files in $ROOT/data\n",
        "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_mTkmDLxpbH"
      },
      "source": [
        "''' IMPORT INITIAL PACKAGES ''' \n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm.auto import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set Numpy Print Options\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHOkvrcenjqV"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6StXmdusxm-N",
        "outputId": "8e106956-4428-4b59-ab25-880c0e939664"
      },
      "source": [
        "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
        "\n",
        "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
        "DATA_ROOT = ROOT + \"data/\"      # Store Datasets\n",
        "CACHE_ROOT = ROOT + \"cache/\"    # Store intermediate cache files\n",
        "LOGS_ROOT = ROOT + \"logs/\"      # Store Logs\n",
        "\n",
        "# Ensure the above directories are present\n",
        "os.chdir(ROOT)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
        "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
        "\n",
        "# Initialize scratch space on /content for faster read-write\n",
        "# Mainly used for temporary file storage\n",
        "SCRATCH_ROOT = '/content/scratch/'   \n",
        "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
        "\n",
        "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
        "! tree -I 'model*|temp__*|wandb*|run_'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n",
            "├── cache\n",
            "│   └── X_dict.pkl\n",
            "├── data\n",
            "│   ├── Boosted_Jets_Sample-0.snappy.parquet\n",
            "│   ├── Boosted_Jets_Sample-1.snappy.parquet\n",
            "│   ├── Boosted_Jets_Sample-2.snappy.parquet\n",
            "│   ├── Boosted_Jets_Sample-3.snappy.parquet\n",
            "│   └── Boosted_Jets_Sample-4.snappy.parquet\n",
            "├── get_dataset.sh\n",
            "├── logs\n",
            "├── nbs\n",
            "│   ├── nb3_OLD.ipynb\n",
            "│   ├── Starter.ipynb\n",
            "│   ├── Week_1.ipynb\n",
            "│   ├── Week_2.ipynb\n",
            "│   ├── Week_3.ipynb\n",
            "│   ├── Week_4.ipynb\n",
            "│   ├── Week_5.ipynb\n",
            "│   └── Week_6.ipynb\n",
            "├── README.md\n",
            "├── requirements.txt\n",
            "└── results\n",
            "\n",
            "5 directories, 17 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnck4o3VBHAY"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryXkYWWy0i9t"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3u5TEbUsTfF"
      },
      "source": [
        "### Dataset Class\n",
        "\n",
        "- We create a Dataset class to feed data to our model.\n",
        "- As we have restrictions on the jets we want to consider (based on number of hits in them), we modify the defauly PyTorch Dataset class.\n",
        "- Additional functions added to the class - \n",
        "    * `get_next_instance`.\n",
        "    * `get_raw_instance`. \n",
        "    * `pad_instance`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFGjbmmkvXJR"
      },
      "source": [
        "''' DEFINE THE DATA CLASS '''\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import *\n",
        "CHANNELS = ['Pt', 'ECal', 'HCal']\n",
        "\n",
        "def parse_img(track_img, reduce=False):\n",
        "    '''\n",
        "    Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
        "    '''\n",
        "    track_img = torch.Tensor(track_img)        \n",
        "    x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
        "    val = track_img[x_pos, y_pos]\n",
        "    out = torch.stack((x_pos,y_pos,val),dim=1)\n",
        "    return out\n",
        "\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
        "        self.parquet = pq.ParquetFile(filename)\n",
        "        self.cur_idx = 0\n",
        "        self.total_len = self.parquet.num_row_groups\n",
        "        self.cols = None \n",
        "        self.verbose = False                # False by default\n",
        "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
        "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
        "        self.allowed_range = range(min_instances, max_instances+1)\n",
        "\n",
        "        self.pt_range = [25,140]\n",
        "        self.return_channels = ['ECal']\n",
        "        self.supress_val = True\n",
        "        self.cached_pts = None\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        raise NotImplementedError('Not needed. Using `get_next_instance` instead.')\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError('Not needed.')\n",
        "\n",
        "    def get_next_instance(self):\n",
        "        '''\n",
        "        - Returns the next valid sample, with it's true index.\n",
        "        - Keeps looping until a valid sample is found.\n",
        "        - A samples validity is based on its `pt_range` (energy range)\n",
        "        as set by the `self.pt_range` attribute.\n",
        "        '''\n",
        "\n",
        "        while True:\n",
        "            for idx in range(self.cur_idx, self.total_len):\n",
        "                raw_jets = self.get_raw_instance(idx)\n",
        "                parsed_jets = [parse_img(j) for j in raw_jets]\n",
        "                gen_pts = [j[:,2] for j in parsed_jets]\n",
        "                self.cached_pts = gen_pts\n",
        "                \n",
        "                if self.supress_val:\n",
        "                    parsed_jets = [j[:,:2] for j in parsed_jets]\n",
        "                \n",
        "                ### Temporary hack for ECal ###\n",
        "                ecal_idx = self.return_channels.index('ECal')\n",
        "                parsed_jet = parsed_jets[ecal_idx]      \n",
        "                gen_pt = gen_pts[ecal_idx].sum() \n",
        "                if self.verbose:\n",
        "                    print('-- Output Shape: {}'.format(parsed_jet.shape))\n",
        "                    print('-- Gen-Pt Val: {}'.format(gen_pt))\n",
        "\n",
        "                # print(gen_pt, self.pt_range)\n",
        "                if (parsed_jet.shape[0] in self.allowed_range) and \\\n",
        "                   (gen_pt >= self.pt_range[0] and gen_pt <= self.pt_range[1]):\n",
        "                    if self.verbose:\n",
        "                        print('-- Returning instance at idx={}'.format(idx))\n",
        "\n",
        "                    padded_jet = self.pad_instance(parsed_jet)\n",
        "                    self.cur_idx = idx + 1\n",
        "                    return padded_jet, idx     # Exit after finding a valid instance\n",
        "                else:\n",
        "                    if self.verbose:\n",
        "                        print('-- Skipped instance at idx={}, shape={}, pt={}'.format(idx, parsed_jet.shape, gen_pt))\n",
        "\n",
        "            # End of dataset, loop back.\n",
        "            self.cur_idx = 0\n",
        "\n",
        "    def get_raw_instance(self, index):\n",
        "        '''\n",
        "        Parses the Parquet dataset to return raw data at a certain index.\n",
        "        '''\n",
        "\n",
        "        c_idx = []\n",
        "        for c in self.return_channels:\n",
        "            assert c in CHANNELS\n",
        "            c_idx.append(CHANNELS.index(c))\n",
        "\n",
        "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
        "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
        "        data['X_jets'] = data['X_jets'][0:]\n",
        "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
        "        raw_jet = dict(data)['X_jets'][c_idx]           # Temporary Hack for Ecal\n",
        "        return raw_jet\n",
        "\n",
        "    def pad_instance(self, instance):\n",
        "        '''\n",
        "        - Pads a data instance (input: `instance`) with additional zeros. \n",
        "        - Shape of the padded instance is made equal to `self.max_instances`. \n",
        "        - Number of hits in the instance is assumed to be in the range: \n",
        "        [self.min_instances, self.max_instances] range.\n",
        "        '''\n",
        "        assert instance.shape[0] <= self.max_instances\n",
        "        assert instance.shape[0] >= self.min_instances\n",
        "        pad_len = self.max_instances - instance.shape[0]\n",
        "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
        "        return instance"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn7qm78X6iGi"
      },
      "source": [
        "\n",
        "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False, onlySave=False, savePath=None):  \n",
        "    '''\n",
        "    Visualise a jet instance.\n",
        "    '''\n",
        "\n",
        "    if not is_parsed:   \n",
        "        arr = parse_img(arr, reduce)\n",
        "\n",
        "    if arr.shape[1] == 3:\n",
        "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
        "    elif arr.shape[1] == 2:\n",
        "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
        "        val = torch.ones_like(x_pos)\n",
        "        scale = None\n",
        "    else:\n",
        "        raise Exception(\"Wrong array dimensions.\")\n",
        "\n",
        "    if scale:\n",
        "        sz = np.array(np.abs(val)) * scale\n",
        "    else:\n",
        "        sz = np.ones_like(val) * 10\n",
        "        \n",
        "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
        "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
        "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
        "    plt.colorbar(sc)\n",
        "    plt.xlim(0, 126)\n",
        "    plt.ylim(0, 126)\n",
        "    plt.xticks(range(0,125,25))\n",
        "    plt.yticks(range(0,125,25))\n",
        "    plt.grid()\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "\n",
        "    if onlySave:\n",
        "        print('Saving to:', savePath)\n",
        "        plt.savefig(savePath, dpi=150)\n",
        "    else:\n",
        "        plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PcRcFj6J25D"
      },
      "source": [
        "### Try using the Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jzIy8YNvXMc"
      },
      "source": [
        "''' INSTANTIATE A DATASET OBJECT  '''\n",
        "\n",
        "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)  # Load the first file\n",
        "dataset = ParquetDataset(dataset_file)             # Instantiate the dataset \n",
        "dataset.verbose = True                             # Set the Verbose flag\n",
        "print('Max Length of Dataset: ', dataset.total_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My4KZ53iSnEA"
      },
      "source": [
        "data_sample, true_idx = dataset.get_next_instance()   # Load an instance\n",
        "data_sample.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JfZOidCvXP4"
      },
      "source": [
        "print(dataset.allowed_range)\n",
        "print()\n",
        "\n",
        "# Load a few instances\n",
        "for i in range(10):\n",
        "    print('[{}]'.format(i))\n",
        "    data_sample, true_idx = dataset.get_next_instance()\n",
        "    print(data_sample.shape, true_idx)\n",
        "    print(dataset.cur_idx)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rJrX4bJQmL7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRXduDWxfwYC"
      },
      "source": [
        "# MASKED AUTOREGRESSIVE FLOWs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvN42-dY7cwl"
      },
      "source": [
        "Adapted from - https://github.com/kamenbliznashki/normalizing_flows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9041zGb0MfKg"
      },
      "source": [
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMwLbC_yf0D5"
      },
      "source": [
        "''' IMPORTS '''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import argparse\n",
        "import pprint\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_2YnZsyBEtX"
      },
      "source": [
        "''' DEFINE THE ARG PARSER '''\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
        "    parser.add_argument('--evaluate', action='store_true', help='Evaluate a flow.')\n",
        "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
        "    parser.add_argument('--generate', action='store_true', help='Generate samples from a model.')\n",
        "    parser.add_argument('--data_dir', default='./data/', help='Location of datasets.')\n",
        "    parser.add_argument('--output_dir', default='./results/run_')\n",
        "    parser.add_argument('--results_file', default='results.txt', help='Filename where to store settings and test results.')\n",
        "    parser.add_argument('--no_cuda', action='store_true', help='Do not use cuda.')\n",
        "    \n",
        "    # data\n",
        "    parser.add_argument('--dataset', default='toy', help='Which dataset to use.')\n",
        "    parser.add_argument('--flip_toy_var_order', action='store_true', help='Whether to flip the toy dataset variable order to (x2, x1).')\n",
        "    parser.add_argument('--seed', type=int, default=1, help='Random seed to use.')\n",
        "    \n",
        "    # model\n",
        "    parser.add_argument('--model', default='maf', help='Which model to use: made, maf.')\n",
        "    \n",
        "    # made parameters\n",
        "    parser.add_argument('--n_blocks', type=int, default=5, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
        "    parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
        "    parser.add_argument('--hidden_size', type=int, default=100, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
        "    parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
        "    parser.add_argument('--activation_fn', type=str, default='relu', help='What activation function to use in the MADEs.')\n",
        "    parser.add_argument('--input_order', type=str, default='sequential', help='What input order to use (sequential | random).')\n",
        "    parser.add_argument('--conditional', default=False, action='store_true', help='Whether to use a conditional model.')\n",
        "    parser.add_argument('--no_batch_norm', action='store_true')\n",
        "    \n",
        "    # training params\n",
        "    parser.add_argument('--batch_size', type=int, default=100)\n",
        "    parser.add_argument('--n_epochs', type=int, default=50)\n",
        "    parser.add_argument('--start_epoch', default=0, help='Starting epoch (for logging; to be overwritten when restoring file.')\n",
        "    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate.')\n",
        "    parser.add_argument('--log_interval', type=int, default=1000, help='How often to show loss statistics and save samples.')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    # Automatically create a new log directory for every run.\n",
        "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
        "    if not os.path.isdir(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # Use Cuda if it's available\n",
        "    args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
        "    args.conditional = False\n",
        "    args.cond_label_size = None\n",
        "\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwpmcNZQqzQ"
      },
      "source": [
        "### Building the MAF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gppl81dWf3fY"
      },
      "source": [
        "''' MODEL CONPONENTS ''' \n",
        "\n",
        "def create_masks(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n",
        "    # MADE paper sec 4:\n",
        "    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n",
        "    degrees = []\n",
        "\n",
        "    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n",
        "    # else init input degrees based on strategy in input_order (sequential or random)\n",
        "    if input_order == 'sequential':\n",
        "        degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
        "        for _ in range(n_hidden + 1):\n",
        "            degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
        "        degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
        "\n",
        "    elif input_order == 'random':\n",
        "        degrees += [torch.randperm(input_size)] if input_degrees is None else [input_degrees]\n",
        "        for _ in range(n_hidden + 1):\n",
        "            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
        "            degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]\n",
        "        min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
        "        degrees += [torch.randint(min_prev_degree, input_size, (input_size,)) - 1] if input_degrees is None else [input_degrees - 1]\n",
        "\n",
        "    # construct masks\n",
        "    masks = []\n",
        "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
        "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
        "\n",
        "    return masks, degrees[0]\n",
        "\n",
        "\n",
        "class MaskedLinear(nn.Linear):\n",
        "    \"\"\" MADE building block layer \"\"\"\n",
        "    def __init__(self, input_size, n_outputs, mask, cond_label_size=None):\n",
        "        super().__init__(input_size, n_outputs)\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        self.cond_label_size = cond_label_size\n",
        "        if cond_label_size is not None:\n",
        "            self.cond_weight = nn.Parameter(torch.rand(n_outputs, cond_label_size) / math.sqrt(cond_label_size))\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        out = F.linear(x, self.weight * self.mask, self.bias)\n",
        "        if y is not None:\n",
        "            out = out + F.linear(y, self.cond_weight)\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        ) + (self.cond_label_size != None) * ', cond_features={}'.format(self.cond_label_size)\n",
        "\n",
        "\n",
        "class LinearMaskedCoupling(nn.Module):\n",
        "    \"\"\" Modified RealNVP Coupling Layers per the MAF paper \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, mask, cond_label_size=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        # scale function\n",
        "        s_net = [nn.Linear(input_size + (cond_label_size if cond_label_size is not None else 0), hidden_size)]\n",
        "        for _ in range(n_hidden):\n",
        "            s_net += [nn.Tanh(), nn.Linear(hidden_size, hidden_size)]\n",
        "        s_net += [nn.Tanh(), nn.Linear(hidden_size, input_size)]\n",
        "        self.s_net = nn.Sequential(*s_net)\n",
        "\n",
        "        # translation function\n",
        "        self.t_net = copy.deepcopy(self.s_net)\n",
        "        # replace Tanh with ReLU's per MAF paper\n",
        "        for i in range(len(self.t_net)):\n",
        "            if not isinstance(self.t_net[i], nn.Linear): self.t_net[i] = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        # apply mask\n",
        "        mx = x * self.mask\n",
        "\n",
        "        # run through model\n",
        "        s = self.s_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        t = self.t_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)  # cf RealNVP eq 8 where u corresponds to x (here we're modeling u)\n",
        "\n",
        "        log_abs_det_jacobian = - (1 - self.mask) * s  # log det du/dx; cf RealNVP 8 and 6; note, sum over input_size done at model log_prob\n",
        "\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        # apply mask\n",
        "        mu = u * self.mask\n",
        "\n",
        "        # run through model\n",
        "        s = self.s_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        t = self.t_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        x = mu + (1 - self.mask) * (u * s.exp() + t)  # cf RealNVP eq 7\n",
        "\n",
        "        log_abs_det_jacobian = (1 - self.mask) * s  # log det dx/du\n",
        "\n",
        "        return x, log_abs_det_jacobian\n",
        "        \n",
        "class BatchNorm(nn.Module):\n",
        "    \"\"\" RealNVP BatchNorm layer \"\"\"\n",
        "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('running_var', torch.ones(input_size))\n",
        "\n",
        "    def forward(self, x, cond_y=None):\n",
        "        if self.training:\n",
        "            self.batch_mean = x.mean(0)\n",
        "            self.batch_var = x.var(0) # note MAF paper uses biased variance estimate; ie x.var(0, unbiased=False)\n",
        "\n",
        "            # update running mean\n",
        "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
        "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
        "\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # compute normalized input (cf original batch norm paper algo 1)\n",
        "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        y = self.log_gamma.exp() * x_hat + self.beta\n",
        "\n",
        "        # compute log_abs_det_jacobian (cf RealNVP paper)\n",
        "        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
        "#        print('in sum log var {:6.3f} ; out sum log var {:6.3f}; sum log det {:8.3f}; mean log_gamma {:5.3f}; mean beta {:5.3f}'.format(\n",
        "#            (var + self.eps).log().sum().data.numpy(), y.var(0).log().sum().data.numpy(), log_abs_det_jacobian.mean(0).item(), self.log_gamma.mean(), self.beta.mean()))\n",
        "        return y, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "    def inverse(self, y, cond_y=None):\n",
        "        if self.training:\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n",
        "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
        "\n",
        "        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
        "\n",
        "        return x, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "\n",
        "class FlowSequential(nn.Sequential):\n",
        "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in self:\n",
        "            x, log_abs_det_jacobian = module(x, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "            # print(\"sum_log_abs_det_jacobians: \", sum_log_abs_det_jacobians)\n",
        "        return x, sum_log_abs_det_jacobians\n",
        "\n",
        "    def inverse(self, u, y):\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in reversed(self):\n",
        "            u, log_abs_det_jacobian = module.inverse(u, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "        return u, sum_log_abs_det_jacobians\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGlvzQxsejFH"
      },
      "source": [
        "''' DEFINING THE MODEL '''\n",
        "\n",
        "class MADE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size -- scalar; dim of inputs\n",
        "            hidden_size -- scalar; dim of hidden layers\n",
        "            n_hidden -- scalar; number of hidden layers\n",
        "            activation -- str; activation function to use\n",
        "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
        "                            or the order flipped from the previous layer in a stack of mades\n",
        "            conditional -- bool; whether model is conditional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # create masks\n",
        "        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n",
        "\n",
        "        # setup activation\n",
        "        if activation == 'relu':\n",
        "            activation_fn = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            activation_fn = nn.Tanh()\n",
        "        else:\n",
        "            raise ValueError('Check activation function.')\n",
        "\n",
        "        # construct model\n",
        "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
        "        self.net = []\n",
        "        for m in masks[1:-1]:\n",
        "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
        "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        # MAF eq 4 -- return mean and log std\n",
        "        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
        "        u = (x - m) * torch.exp(-loga)\n",
        "        # MAF eq 5\n",
        "        log_abs_det_jacobian = - loga\n",
        "        # print(u, log_abs_det_jacobian)\n",
        "        # print(\"-----------\\n\")\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n",
        "        # MAF eq 3\n",
        "        D = u.shape[1]\n",
        "        x = torch.zeros_like(u)\n",
        "        # run through reverse model\n",
        "        for i in self.input_degrees:\n",
        "            m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
        "            x[:,i] = u[:,i] * torch.exp(loga[:,i]) + m[:,i]\n",
        "        log_abs_det_jacobian = loga\n",
        "        return x, log_abs_det_jacobian\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        u, log_abs_det_jacobian = self.forward(x, y)\n",
        "        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)\n",
        "\n",
        "\n",
        "class MAF(nn.Module):\n",
        "    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', batch_norm=True):\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # construct model\n",
        "        modules = []\n",
        "        self.input_degrees = None\n",
        "        for i in range(n_blocks):\n",
        "            modules += [MADE(input_size, hidden_size, n_hidden, cond_label_size, activation, input_order, self.input_degrees)]\n",
        "            self.input_degrees = modules[-1].input_degrees.flip(0)\n",
        "            modules += batch_norm * [BatchNorm(input_size)]\n",
        "\n",
        "        # print(modules)\n",
        "        self.net = FlowSequential(*modules)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        return self.net(x, y)\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        return self.net.inverse(u, y)\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        # This function breaks up during training and might require more work.\n",
        "        # The try-except statement has been added to continue training in that case.\n",
        "        try:\n",
        "            u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
        "            # print(\"[Log Prob] u: \", u.shape)\n",
        "            # print(\"[Log Prob] Sum Log Abs Det J: \", sum_log_abs_det_jacobians.shape)\n",
        "            v =  torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)\n",
        "        except:\n",
        "            print(\"--- [ Encountered NaN ]\")\n",
        "            \n",
        "            v = torch.sum(torch.ones(u.shape, requires_grad=True) * 1e-2)\n",
        "        # print(\"[Log Prob] v: \", v.shape)\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ-HD7Ksf3c_"
      },
      "source": [
        "''' DEFINING THE TRAINING FUNCTIONS ''' \n",
        "\n",
        "def get_batch(dataset, bs, return_indices=False):\n",
        "    '''\n",
        "        Get a new batch of input data from the `dataset` object of batch size `bs`. \n",
        "        \n",
        "        Also returns the actual indices of the data instances in the batch \n",
        "        (corresponding to the Parquet files) if `return_indices` is set to true. \n",
        "    '''\n",
        "    input_batch = []\n",
        "    true_indices = []\n",
        "    for batch_idx in range(bs):\n",
        "        input_data, true_index = dataset.get_next_instance()\n",
        "        input_data = input_data.reshape(-1)\n",
        "        input_batch.append(input_data)\n",
        "        true_indices.append(true_index)\n",
        "    input_batch = torch.stack(input_batch)\n",
        "    # print(input_batch.shape)\n",
        "\n",
        "    if return_indices:\n",
        "        return input_batch, true_indices\n",
        "    else:\n",
        "        return input_batch\n",
        "\n",
        "def train(model, dataset, optimizer, args):\n",
        "    '''\n",
        "        Trains the input `model` on the given `dataset`, using the provided `optimizer`.\n",
        "\n",
        "        The `args` parameter should contain the parsed training arguments from \n",
        "        the `get_args()` function.\n",
        "\n",
        "        Note: It is recommended to instead use the `train_evaluate()` function \n",
        "        as it contains additional testing and logging functionality.  \n",
        "    '''\n",
        "    init_steps = args.step\n",
        "    for i in range(args.n_steps):\n",
        "        model.train()\n",
        "        x = get_batch(dataset, args.batch_size).to(args.device)\n",
        "        # print(x.shape)\n",
        "        loss = - model.log_prob(x, None).mean(0)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({'Step': args.step, \n",
        "            'Loss': loss.item(), \n",
        "            'Learning_Rate': args.lr})\n",
        "        \n",
        "        if i % args.log_interval == 0:\n",
        "            print('Iter {:4d} / {:4d} | loss {:.4f}'.format(init_steps + i, init_steps + args.n_steps, loss.item())) \n",
        "\n",
        "\n",
        "def train_batch(model, x_batch, optimizer):\n",
        "    '''\n",
        "        Train a single batch. Designed for use from within the \n",
        "        `train_evaluate()` function.\n",
        "    '''\n",
        "\n",
        "    model.train()\n",
        "    # print(x_batch.shape)\n",
        "    loss = - model.log_prob(x_batch, None).mean(0)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_batch(model, x_batch, iter, args):\n",
        "    '''\n",
        "        Evaluate a single batch. Designed for use from within the \n",
        "        `train_evaluate()` function.\n",
        "    '''\n",
        "    model.eval()\n",
        "    # print(x_batch.shape)\n",
        "    x_batch = x_batch.to(args.device)  ## To edit\n",
        "    logprobs = model.log_prob(x_batch)\n",
        "    # print(logprobs)\n",
        "    logprobs = logprobs.to(args.device)\n",
        "    logprob_mean, logprob_std = logprobs.mean(0), 2 * logprobs.var(0).sqrt() / math.sqrt(args.batch_size)\n",
        "    return logprob_mean, logprob_std\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def generate_samples(model, args, iter='?', toSave=False):\n",
        "#     '''\n",
        "#         Generate a new samples from a trained model. \n",
        "#     '''\n",
        "\n",
        "#     model.eval()\n",
        "#     u = model.base_dist.sample((1, args.n_components)).squeeze()\n",
        "#     samples, _ = model.inverse(u)\n",
        "#     log_probs = model.log_prob(samples).sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
        "#     samples = samples[log_probs]\n",
        "\n",
        "#     if toSave:\n",
        "#         print(samples.shape)\n",
        "#         # samples = samples.view(samples.shape[0], *args.input_dims)\n",
        "#         # # samples = (torch.sigmoid(samples) - dataset_lam) / (1 - 2 * dataset_lam)\n",
        "#         # filename = 'generated_samples' + '_epoch_{}'.format(iter) + '.png'\n",
        "#         # save_image(samples, os.path.join(args.output_dir, filename), nrow=1, normalize=True)\n",
        "\n",
        "#     return samples\n",
        "\n",
        "def save_model(model, optimizer, iter, args, save_name='model_state.pt'):\n",
        "    '''\n",
        "        Cache the training state consisting of the `model`, `optimizer` and current \n",
        "        training interation - `iter`. \n",
        "\n",
        "        Saved model can be loaded using the `load_model()` function. \n",
        "    '''\n",
        "    save_dict = {'iteration': iter,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict()}\n",
        "    save_path = os.path.join(args.output_dir, save_name)\n",
        "    torch.save(save_dict, save_path)\n",
        "\n",
        "def load_model(model, optimizer, args, restore_path):\n",
        "    '''\n",
        "        Load a model cached using the `save_model()` function. \n",
        "        Argument `restore_path` is expected to be a `.pt` file.\n",
        "    '''\n",
        "    state = torch.load(restore_path, map_location=args.device)\n",
        "    model.load_state_dict(state['model_state'])\n",
        "    optimizer.load_state_dict(state['optimizer_state'])\n",
        "    args.step = state['iteration'] + 1\n",
        "    return model, optimizer, args\n",
        "\n",
        "def train_evaluate(model, optimizer, train_dataset, test_dataset, args):\n",
        "    '''\n",
        "        Root function to Train and Evaluate the MAF model. \n",
        "\n",
        "        Supports functionality to log training and testing on the console and \n",
        "        onto Weights and Biases (wandb) as well as checkpoint models based on \n",
        "        test set perfomance. \n",
        "    '''\n",
        "\n",
        "    best_eval_logprob = float('-inf')\n",
        "    init_steps = args.step\n",
        "    \n",
        "    for i in range(args.n_steps):\n",
        "        training_batch = get_batch(train_dataset, args.batch_size).to(args.device)\n",
        "        loss = train_batch(model, training_batch, optimizer)\n",
        "        \n",
        "        testing_batch = get_batch(test_dataset, args.batch_size).to(args.device)\n",
        "        logprob_mean, logprob_std = evaluate_batch(model, testing_batch, iter, args)\n",
        "        # print('=====================================')\n",
        "        # print(logprob_mean)\n",
        "        # print('=====================================')\n",
        "        # print(logprob_std)\n",
        "        # print('=====================================')\n",
        "\n",
        "        wandb.log({'Step': args.step, \n",
        "                   'Loss': loss.item(), \n",
        "                   'Learning_Rate': args.lr,\n",
        "                   'LogProb_Mean': logprob_mean, \n",
        "                   'LogProb_Std': logprob_std})\n",
        "\n",
        "        if i % args.log_interval == 0:\n",
        "            print_output = '[Iter: {:4d}/{:4d}] '.format(init_steps + i, init_steps + args.n_steps)\n",
        "            print_output += 'Loss: {:.3f}'.format(loss.item())\n",
        "            print_output += '  |  LogP(x): {:.3f} +/- {:.3f}'.format(logprob_mean, logprob_std)                       \n",
        "            print(print_output)\n",
        "\n",
        "            save_model(model, optimizer, i, args, save_name='model_state.pt')\n",
        "            if logprob_mean > best_eval_logprob:\n",
        "                best_eval_logprob = logprob_mean\n",
        "                save_model(model, optimizer, i, args, save_name='best_model_checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjxo74OwQ59P"
      },
      "source": [
        "### Setting up the Model for Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0rsBLi60gMI"
      },
      "source": [
        "''' INITIAL WANDB CONFIG '''\n",
        "\n",
        "DEFAULT_CFG = {\n",
        "    'model': 'MAF_Valid-Samples',\n",
        "    'root_dir': ROOT,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtdJUtDxS8Mb"
      },
      "source": [
        "''' LOAD TEST/TRAIN DATASETS '''\n",
        "\n",
        "def load_dataset(index, pt_range=[50,55], data_root=DATA_ROOT, input_format=INPUT_FORMAT):\n",
        "    '''\n",
        "        Loads the `index` dataset file and returns a `ParquetDataset` object \n",
        "        containing the instances in that file.\n",
        "    '''\n",
        "    dataset_file = data_root + input_format.format(index)\n",
        "    dataset = ParquetDataset(dataset_file)\n",
        "    dataset.pt_range = pt_range\n",
        "    return dataset\n",
        "\n",
        "# Initialize Datasets\n",
        "PT_RANGE = [50,55]\n",
        "train_dataset = load_dataset(0, pt_range=PT_RANGE)\n",
        "test_dataset = load_dataset(1, pt_range=PT_RANGE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY7zIDZthVMy"
      },
      "source": [
        "''' TRAINING SETUP '''\n",
        "\n",
        "# Filter training warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Get Args\n",
        "args = get_args()\n",
        "\n",
        "# Set custom args\n",
        "args.input_dims = None\n",
        "args.input_size = train_dataset.max_instances * 2    # Can be tuned\n",
        "args.hidden_size = 300   # Can be tuned\n",
        "args.n_blocks = 8\n",
        "args.n_components = 1\n",
        "args.n_hidden = 1\n",
        "args.n_steps = 5000       # Iterations to train for.\n",
        "args.step = 0             # Initial training iteration.\n",
        "args.lr = 1e-4            # Learning Rate\n",
        "args.batch_size = 32      # Batch Size\n",
        "args.cuda = 0             # GPU index\n",
        "args.log_interval = 1     # Logging Interval (based on iteration frequency)\n",
        "args.pt_range = PT_RANGE  # Range of energy to filter jets\n",
        "args.no_batch_norm = False\n",
        "\n",
        "# Set Seeds\n",
        "torch.manual_seed(args.seed)\n",
        "if args.device.type == 'cuda': \n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Get the MAF Model\n",
        "model = MAF(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.cond_label_size,\n",
        "                    args.activation_fn, args.input_order, batch_norm=not args.no_batch_norm)\n",
        "model = model.to(args.device)\n",
        "\n",
        "# Save Config\n",
        "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
        "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
        "            'Model:\\n{}'.format(model)\n",
        "config_path = os.path.join(args.output_dir, 'config.txt')\n",
        "if not os.path.exists(config_path):\n",
        "    with open(config_path, 'a') as f:\n",
        "        print(config, file=f)\n",
        "\n",
        "# Get Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g21d2kMrGD_q"
      },
      "source": [
        "''' INITIALIZE A WANDB RUN '''\n",
        "\n",
        "# Init Wandb\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
        "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
        "wandb.config.update(args)\n",
        "\n",
        "# Save files for later\n",
        "! pip freeze > requirements.txt\n",
        "# wandb.save(ROOT + 'requirements.txt')       # TODO: Fix permission issue on Colab.\n",
        "# wandb.save(config_path)                     # TODO: Fix permission issue on Colab."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVZddE2HSddx"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arju5UwogcQs",
        "outputId": "34f684de-c01a-4ec6-dcf9-6bdfd6f8773f"
      },
      "source": [
        "''' TRAIN + EVALUATE THE MODEL '''\n",
        "\n",
        "train_evaluate(model, optimizer, train_dataset, test_dataset, args, toGenerate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- [ Encountered NaN ]\n",
            "[Iter:    0/5000] Loss: 7335.718  |  LogP(x): 491.520 +/- nan\n",
            "[Iter:    1/5000] Loss: 7206.878  |  LogP(x): -20063.887 +/- 1499.554\n",
            "[Iter:    2/5000] Loss: 7201.562  |  LogP(x): -14998.738 +/- 3175.619\n",
            "[Iter:    3/5000] Loss: 6909.108  |  LogP(x): -12103.184 +/- 1325.232\n",
            "[Iter:    4/5000] Loss: 7181.210  |  LogP(x): -10597.162 +/- 545.477\n",
            "[Iter:    5/5000] Loss: 7169.951  |  LogP(x): -9496.451 +/- 375.776\n",
            "[Iter:    6/5000] Loss: 6763.291  |  LogP(x): -9188.175 +/- 477.719\n",
            "[Iter:    7/5000] Loss: 6955.591  |  LogP(x): -8744.181 +/- 386.556\n",
            "[Iter:    8/5000] Loss: 7095.173  |  LogP(x): -8299.588 +/- 232.408\n",
            "[Iter:    9/5000] Loss: 6568.482  |  LogP(x): -8039.018 +/- 170.622\n",
            "[Iter:   10/5000] Loss: 7084.449  |  LogP(x): -7799.437 +/- 190.473\n",
            "[Iter:   11/5000] Loss: 6847.776  |  LogP(x): -7980.133 +/- 232.192\n",
            "[Iter:   12/5000] Loss: 6975.272  |  LogP(x): -7869.228 +/- 166.644\n",
            "[Iter:   13/5000] Loss: 6709.409  |  LogP(x): -7738.040 +/- 215.357\n",
            "[Iter:   14/5000] Loss: 6631.835  |  LogP(x): -7741.592 +/- 356.003\n",
            "[Iter:   15/5000] Loss: 6375.024  |  LogP(x): -7655.049 +/- 345.404\n",
            "[Iter:   16/5000] Loss: 6445.288  |  LogP(x): -7590.798 +/- 225.835\n",
            "[Iter:   17/5000] Loss: 6686.616  |  LogP(x): -7530.656 +/- 218.229\n",
            "[Iter:   18/5000] Loss: 6692.167  |  LogP(x): -7470.928 +/- 295.127\n",
            "[Iter:   19/5000] Loss: 6590.315  |  LogP(x): -7251.589 +/- 123.556\n",
            "[Iter:   20/5000] Loss: 6589.625  |  LogP(x): -7359.690 +/- 194.645\n",
            "[Iter:   21/5000] Loss: 6505.430  |  LogP(x): -7214.591 +/- 316.434\n",
            "[Iter:   22/5000] Loss: 6771.669  |  LogP(x): -7752.443 +/- 735.812\n",
            "[Iter:   23/5000] Loss: 6702.815  |  LogP(x): -7116.055 +/- 107.897\n",
            "[Iter:   24/5000] Loss: 6543.222  |  LogP(x): -7336.514 +/- 401.528\n",
            "[Iter:   25/5000] Loss: 6823.938  |  LogP(x): -7211.165 +/- 261.885\n",
            "[Iter:   26/5000] Loss: 6757.773  |  LogP(x): -7132.787 +/- 136.866\n",
            "[Iter:   27/5000] Loss: 6877.063  |  LogP(x): -7423.477 +/- 691.171\n",
            "[Iter:   28/5000] Loss: 6842.435  |  LogP(x): -7094.533 +/- 156.922\n",
            "[Iter:   29/5000] Loss: 6681.322  |  LogP(x): -7068.166 +/- 150.296\n",
            "[Iter:   30/5000] Loss: 6445.041  |  LogP(x): -7075.146 +/- 139.329\n",
            "[Iter:   31/5000] Loss: 6726.113  |  LogP(x): -7478.431 +/- 1051.442\n",
            "[Iter:   32/5000] Loss: 6755.272  |  LogP(x): -7028.333 +/- 197.899\n",
            "[Iter:   33/5000] Loss: 6634.817  |  LogP(x): -7103.398 +/- 591.177\n",
            "[Iter:   34/5000] Loss: 6675.854  |  LogP(x): -6991.438 +/- 152.312\n",
            "[Iter:   35/5000] Loss: 6437.353  |  LogP(x): -7557.896 +/- 845.188\n",
            "[Iter:   36/5000] Loss: 6412.162  |  LogP(x): -6969.241 +/- 154.715\n",
            "[Iter:   37/5000] Loss: 6821.722  |  LogP(x): -7034.573 +/- 202.375\n",
            "[Iter:   38/5000] Loss: 6528.572  |  LogP(x): -6824.778 +/- 118.715\n",
            "[Iter:   39/5000] Loss: 6375.583  |  LogP(x): -6991.948 +/- 231.875\n",
            "[Iter:   40/5000] Loss: 6451.175  |  LogP(x): -6851.236 +/- 113.202\n",
            "[Iter:   41/5000] Loss: 5827.071  |  LogP(x): -6942.954 +/- 221.323\n",
            "[Iter:   42/5000] Loss: 6559.073  |  LogP(x): -6858.913 +/- 159.397\n",
            "[Iter:   43/5000] Loss: 6696.210  |  LogP(x): -6958.787 +/- 187.290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdcELfbTkpk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQLjVJd1TlDP"
      },
      "source": [
        "### Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-LUx3osTkgo"
      },
      "source": [
        "''' MANUALLY SAVE THE MODEL '''\n",
        "\n",
        "# save_model(model, optimizer, i=-1, args=args, save_name='model_state.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCmOpFdTkY7"
      },
      "source": [
        "''' LOAD A MODEL '''\n",
        "\n",
        "# RESTORE_PATH = '/content/drive/MyDrive/_GSoC/Normalizing-Flows/results/run_/2021-08-20_08-22-09/best_model_checkpoint.pt'\n",
        "# RESTORE_PATH = '/content/drive/MyDrive/_GSoC/Normalizing-Flows/results/run_/2021-08-20_08-22-09/model_state.pt'\n",
        "RESTORE_PATH = '/content/drive/MyDrive/_GSoC/Normalizing-Flows/results/run_/2021-08-20_11-19-50/best_model_checkpoint.pt'\n",
        "\n",
        "# model, optimizer, args = load_model(model, optimizer, args, RESTORE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5V25shYtrdA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P4pbubtShBo"
      },
      "source": [
        "### Generating New Instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_GBBiy_trXq"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize(instance):\n",
        "    '''\n",
        "        Normalize the input instance between [0,125]\n",
        "    '''\n",
        "    scaler = MinMaxScaler()\n",
        "    instance = scaler.fit_transform(instance) * 125\n",
        "    return torch.Tensor(instance)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_sample(model, args, \n",
        "                    sample_size=4, \n",
        "                    save_size=1, \n",
        "                    toNormalize=True,\n",
        "                    iter=-1, toSave=False):\n",
        "    '''\n",
        "        Generate new samples from a trained model. `sample_size` defines the \n",
        "        number of samples to generate out of which `save_size` number of samples\n",
        "        are saved into the current runs logging folder (Samples with the highest\n",
        "        Log Probability are saved). \n",
        "    '''\n",
        "\n",
        "    args.batch_size = 1     # Needed to avoid CUDA out of memory.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        u = model.base_dist.sample((sample_size, args.n_components)).reshape(sample_size, -1)\n",
        "        samples, _ = model.inverse(u)\n",
        "        log_probs_raw = model.log_prob(samples)\n",
        "        order = log_probs_raw.sort(0)[1].flip(0)  # sort by log_prob; take argsort idxs; flip high to low\n",
        "        print('Sample Probability: ', log_probs_raw[order])\n",
        "        samples = samples.detach().cpu()\n",
        "        samples = samples[order][:save_size]\n",
        "        samples = ([s.reshape(args.input_size//2, 2) for s in samples])\n",
        "\n",
        "        # Normalize the samples? \n",
        "        if toNormalize:\n",
        "            samples = ([normalize(s) for s in samples])\n",
        "\n",
        "        # Save the samples to Disk? \n",
        "        if toSave:\n",
        "            for i in range(len(samples)):\n",
        "                file_name = 'generated_samples_iter={}_sample={}_.png'.format(iter, i)\n",
        "                file_path = os.path.join(args.output_dir, file_name)\n",
        "                title = 'Pred_Idx={}'.format(i)\n",
        "                vis(samples[i], title=title, onlySave=True, savePath=file_path)\n",
        "\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9Qqvi7s17Zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998d07fa-ee98-4e22-f31e-81147ac937d1"
      },
      "source": [
        "preds = generate_sample(model, args, sample_size=1024, save_size=4, toNormalize=False, toSave=True)\n",
        "\n",
        "# for i in range(len(preds)):\n",
        "#     vis(preds[i], title='Pred_Idx={}'.format(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Probablities:  tensor([-5463.1377, -5463.2837, -5468.6416,  ..., -5644.5547, -5644.9629,\n",
            "        -5675.0122], device='cuda:0')\n",
            "Saving to: ./results/run_/2021-08-20_11-19-50/generated_samples_iter=-1_sample=0_.png\n",
            "Saving to: ./results/run_/2021-08-20_11-19-50/generated_samples_iter=-1_sample=1_.png\n",
            "Saving to: ./results/run_/2021-08-20_11-19-50/generated_samples_iter=-1_sample=2_.png\n",
            "Saving to: ./results/run_/2021-08-20_11-19-50/generated_samples_iter=-1_sample=3_.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo8I96nJCFMM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFO-69q0trMH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdnuJY5Wgb57"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Iozqt_UqW4-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sJIqEp6qWZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfiaE0u8qWDh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLbTRPGlgbXz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbO-Hzke7GJ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0LVpcOe7GSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}