{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa9e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_DESC = 'Setting up BNAF.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d272ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed1ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__ == '1.9.0+cu102'\n",
    "! pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213f2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04589e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLAB ###\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages\n",
    "! apt-get install tree >/dev/null\n",
    "\n",
    "# Download dataset\n",
    "# ! ./get_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a756d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format of files in $ROOT/data\n",
    "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ac39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c23e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*|wandb*|run_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33aaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=1024):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of hits to force in each jet.\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        jets = [self.parse_img(dict(data)['X_jets'][i]) for i in self.channels]\n",
    "        jets = jets[0]            # Temporary Hack for Ecal\n",
    "        return jets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)\n",
    "        # print(out.shape)\n",
    "\n",
    "        if self.max_instances:\n",
    "            # Pad instances with fewer hits\n",
    "            if out.shape[0] <= self.max_instances:\n",
    "                pad_len = self.max_instances - out.shape[0]\n",
    "                out = F.pad(out, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "            # Cut off excess hits from other jets\n",
    "            else:\n",
    "                out = out[:self.max_instances,:]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Output Shape:', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ad3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1447809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances=None)\n",
    "dataset.verbose = True\n",
    "print('Length of Dataset: ', dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91aa3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df10ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ac8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d77a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])"
     ]
    }
   ],
   "source": [
    "d = [1,2,3,4,5]\n",
    "d = torch.Tensor(d)\n",
    "d = d.type(torch.int)\n",
    "d = d.type(torch.float32)\n",
    "d\n",
    "# d.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1841e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        # print(x.dtype)\n",
    "        # print(w.dtype)\n",
    "        # print(x)\n",
    "        # print(w)\n",
    "        x = x.type(torch.float32)\n",
    "        # print(x.dtype)\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbee0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9049a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    for idx in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.__getitem__(idx)\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item()})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b18ddf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1586c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048   #3072\n",
    "args.n_steps = 500\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.3\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481ef55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_063355-pbccpm56/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_063355-pbccpm56/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ef1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b780ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa888c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bnaf(model, dataset, instance_idx, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,3).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f26b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bnaf(model, dataset, instance_idx, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efd33cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e05a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2])"
     ]
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3671ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56d88511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if input_data.shape[1] == 2:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    else:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00e92d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2ff4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea4d50cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "132a4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if input_data.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    else:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a145deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b791bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if input_data.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif input_data.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5357c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d9618de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b4b5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03ee17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = input_data\n",
    "\n",
    "\n",
    "x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "val = np.ones_like(x_pos)\n",
    "scale = None\n",
    "\n",
    "sz = np.ones_like(val) * 10\n",
    "    \n",
    "plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "plt.colorbar(sc)\n",
    "plt.xlim(0, 126)\n",
    "plt.ylim(0, 126)\n",
    "plt.xticks(range(0,125,25))\n",
    "plt.yticks(range(0,125,25))\n",
    "plt.grid()\n",
    "if title:\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99d2b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = input_data\n",
    "\n",
    "\n",
    "x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "val = np.ones_like(x_pos)\n",
    "scale = None\n",
    "\n",
    "sz = np.ones_like(val) * 10\n",
    "    \n",
    "plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "cm = plt.cm.get_cmap('gist_heat')     # 'gist_heat' / 'YlOrRd'\n",
    "sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "plt.colorbar(sc)\n",
    "plt.xlim(0, 126)\n",
    "plt.ylim(0, 126)\n",
    "plt.xticks(range(0,125,25))\n",
    "plt.yticks(range(0,125,25))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5c31772",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = input_data\n",
    "\n",
    "x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "val = np.ones_like(x_pos)\n",
    "scale = None\n",
    "\n",
    "sz = np.ones_like(val) * 10\n",
    "print(x_pos.shape, y_pos.shape, val.shape)\n",
    "\n",
    "plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "cm = plt.cm.get_cmap('gist_heat')     # 'gist_heat' / 'YlOrRd'\n",
    "sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "plt.colorbar(sc)\n",
    "plt.xlim(0, 126)\n",
    "plt.ylim(0, 126)\n",
    "plt.xticks(range(0,125,25))\n",
    "plt.yticks(range(0,125,25))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc4207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3, 10, 22,  ...,  0,  0,  0])"
     ]
    }
   ],
   "source": [
    "x_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fed4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80ed54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*|wandb*|run_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2efd898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = input_data\n",
    "\n",
    "x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "val = np.ones_like(x_pos)\n",
    "scale = None\n",
    "\n",
    "sz = np.ones_like(val) * 10\n",
    "print(x_pos.shape, y_pos.shape, val.shape)\n",
    "\n",
    "plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "cm = plt.cm.get_cmap('gist_heat')     # 'gist_heat' / 'YlOrRd'\n",
    "sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "plt.colorbar(sc)\n",
    "plt.xlim(0, 126)\n",
    "plt.ylim(0, 126)\n",
    "plt.xticks(range(0,125,25))\n",
    "plt.yticks(range(0,125,25))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2c6986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35aff069",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74def7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c0e7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y', 'Energy']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b43d53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49e64b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = abs(pred)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97040b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "p = scaler.fit_transform(abs(pred)))\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08a7c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "p = scaler.fit_transform(abs(pred))\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51e4427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# p = scaler.fit_transform(abs(pred))\n",
    "\n",
    "print(p.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d1aa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# p = scaler.fit_transform(abs(pred))\n",
    "\n",
    "print(p.max(), m.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59e27e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# p = scaler.fit_transform(abs(pred))\n",
    "\n",
    "print(p.max(), p.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8cf25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "p = scaler.fit_transform(abs(pred))\n",
    "\n",
    "print(p.max(), p.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbcd8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "p = scaler.fit_transform(abs(pred)) * 125\n",
    "\n",
    "print(p.max(), p.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a5e69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e0e3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c252c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83324799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "p = scaler.fit_transform(abs(pred)) * 125\n",
    "p = torch.Tensor(p)\n",
    "\n",
    "print(p.max(), p.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b31d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30fb41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f917fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58883d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d117975",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ee07c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11008ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=1, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17c1bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6ca2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "503a49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ad1e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i])\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c7aa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(input_data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(input_data[:,i], label='labels')\n",
    "        ax.plot(pred[:,i], label='prediction')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3a45d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(input_data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(input_data[:,i], label='labels')\n",
    "        ax.plot(pred[:,i], label='prediction')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "772b0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i], label='prediction', col='lightskyblue')\n",
    "        ax.plot(data[:,i], label='labels')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da821a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data[:,i], label='labels')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a83b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c880d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f03c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0])"
     ]
    }
   ],
   "source": [
    "input_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84277a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [1,2,3,4,0,0,0]\n",
    "k.nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "755d302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function Tensor.nonzero>"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([1,2,3,4,0,0,0])\n",
    "k.nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b01656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]])"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([1,2,3,4,0,0,0])\n",
    "k.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b2c2763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([1,2,3,4,0,0,0])\n",
    "len(k.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49a17ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        ax.plot(pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27768694",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1024 / 300) * list(range(0,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7296bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([   0.   ,    3.413,    6.827, ..., 3485.013, 3488.427, 3491.84 ])"
     ]
    }
   ],
   "source": [
    "(1024 / 300) * np.array(range(0,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39d89323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([   0.   ,    3.413,    6.827,   10.24 ,   13.653,   17.067,\n",
      "         20.48 ,   23.893,   27.307,   30.72 ,   34.133,   37.547,\n",
      "         40.96 ,   44.373,   47.787,   51.2  ,   54.613,   58.027,\n",
      "         61.44 ,   64.853,   68.267,   71.68 ,   75.093,   78.507,\n",
      "         81.92 ,   85.333,   88.747,   92.16 ,   95.573,   98.987,\n",
      "        102.4  ,  105.813,  109.227,  112.64 ,  116.053,  119.467,\n",
      "        122.88 ,  126.293,  129.707,  133.12 ,  136.533,  139.947,\n",
      "        143.36 ,  146.773,  150.187,  153.6  ,  157.013,  160.427,\n",
      "        163.84 ,  167.253,  170.667,  174.08 ,  177.493,  180.907,\n",
      "        184.32 ,  187.733,  191.147,  194.56 ,  197.973,  201.387,\n",
      "        204.8  ,  208.213,  211.627,  215.04 ,  218.453,  221.867,\n",
      "        225.28 ,  228.693,  232.107,  235.52 ,  238.933,  242.347,\n",
      "        245.76 ,  249.173,  252.587,  256.   ,  259.413,  262.827,\n",
      "        266.24 ,  269.653,  273.067,  276.48 ,  279.893,  283.307,\n",
      "        286.72 ,  290.133,  293.547,  296.96 ,  300.373,  303.787,\n",
      "        307.2  ,  310.613,  314.027,  317.44 ,  320.853,  324.267,\n",
      "        327.68 ,  331.093,  334.507,  337.92 ,  341.333,  344.747,\n",
      "        348.16 ,  351.573,  354.987,  358.4  ,  361.813,  365.227,\n",
      "        368.64 ,  372.053,  375.467,  378.88 ,  382.293,  385.707,\n",
      "        389.12 ,  392.533,  395.947,  399.36 ,  402.773,  406.187,\n",
      "        409.6  ,  413.013,  416.427,  419.84 ,  423.253,  426.667,\n",
      "        430.08 ,  433.493,  436.907,  440.32 ,  443.733,  447.147,\n",
      "        450.56 ,  453.973,  457.387,  460.8  ,  464.213,  467.627,\n",
      "        471.04 ,  474.453,  477.867,  481.28 ,  484.693,  488.107,\n",
      "        491.52 ,  494.933,  498.347,  501.76 ,  505.173,  508.587,\n",
      "        512.   ,  515.413,  518.827,  522.24 ,  525.653,  529.067,\n",
      "        532.48 ,  535.893,  539.307,  542.72 ,  546.133,  549.547,\n",
      "        552.96 ,  556.373,  559.787,  563.2  ,  566.613,  570.027,\n",
      "        573.44 ,  576.853,  580.267,  583.68 ,  587.093,  590.507,\n",
      "        593.92 ,  597.333,  600.747,  604.16 ,  607.573,  610.987,\n",
      "        614.4  ,  617.813,  621.227,  624.64 ,  628.053,  631.467,\n",
      "        634.88 ,  638.293,  641.707,  645.12 ,  648.533,  651.947,\n",
      "        655.36 ,  658.773,  662.187,  665.6  ,  669.013,  672.427,\n",
      "        675.84 ,  679.253,  682.667,  686.08 ,  689.493,  692.907,\n",
      "        696.32 ,  699.733,  703.147,  706.56 ,  709.973,  713.387,\n",
      "        716.8  ,  720.213,  723.627,  727.04 ,  730.453,  733.867,\n",
      "        737.28 ,  740.693,  744.107,  747.52 ,  750.933,  754.347,\n",
      "        757.76 ,  761.173,  764.587,  768.   ,  771.413,  774.827,\n",
      "        778.24 ,  781.653,  785.067,  788.48 ,  791.893,  795.307,\n",
      "        798.72 ,  802.133,  805.547,  808.96 ,  812.373,  815.787,\n",
      "        819.2  ,  822.613,  826.027,  829.44 ,  832.853,  836.267,\n",
      "        839.68 ,  843.093,  846.507,  849.92 ,  853.333,  856.747,\n",
      "        860.16 ,  863.573,  866.987,  870.4  ,  873.813,  877.227,\n",
      "        880.64 ,  884.053,  887.467,  890.88 ,  894.293,  897.707,\n",
      "        901.12 ,  904.533,  907.947,  911.36 ,  914.773,  918.187,\n",
      "        921.6  ,  925.013,  928.427,  931.84 ,  935.253,  938.667,\n",
      "        942.08 ,  945.493,  948.907,  952.32 ,  955.733,  959.147,\n",
      "        962.56 ,  965.973,  969.387,  972.8  ,  976.213,  979.627,\n",
      "        983.04 ,  986.453,  989.867,  993.28 ,  996.693, 1000.107,\n",
      "       1003.52 , 1006.933, 1010.347, 1013.76 , 1017.173, 1020.587])"
     ]
    }
   ],
   "source": [
    "(1024 / 300) * np.array(range(0,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e808676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d18f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    data[data.nonzero()]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e12cf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    print(n_hits)\n",
    "    data[data.nonzero()]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7c5f29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    print(n_hits)\n",
    "    data = data[data.nonzero()]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "450d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    data = data[data.nonzero()]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        # print(data_idx.shape, data.shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0055a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    data = data[data.nonzero()]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c0295c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[data.nonzero()]\n",
    "    print(data.shape)\n",
    "    return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f54b8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[:,:data.nonzero()]\n",
    "    print(data.shape)\n",
    "    return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9896c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[:,:n_hits]\n",
    "    print(data.shape)\n",
    "    return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c1636b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[:n_hits,:]\n",
    "    print(data.shape)\n",
    "    return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7e0774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[:n_hits]\n",
    "    print(data.shape)\n",
    "    return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e020a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    # print(n_hits)\n",
    "    print(data.shape)\n",
    "    data = data[:n_hits]\n",
    "    print(data.shape)\n",
    "    # return\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        print(data_idx.shape, data.shape)\n",
    "        print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d07073e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero())\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        # print(data_idx.shape, data.shape)\n",
    "        # print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86aa3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        # print(data_idx.shape, data.shape)\n",
    "        # print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c66de2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) / 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        # print(data_idx.shape, data.shape)\n",
    "        # print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e334e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "        # print(data_idx.shape, data.shape)\n",
    "        # print(pred[:,i].shape, data[:,i].shape)\n",
    "        ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "        ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "00f16648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, sorted=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if sorted:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "987d987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, to_sort=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if to_sort:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, to_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bb80cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77d02332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a0d0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b52f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "63a98cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, to_sort=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if to_sort:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, to_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4a5695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048*2   #3072\n",
    "args.n_steps = 500*2\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3d7ade93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_073653-1uk1de3x/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_073653-1uk1de3x/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d3392be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2933e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9ca229ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "65230583",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06cb0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a08269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, to_sort=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if to_sort:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, to_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03faab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000"
     ]
    }
   ],
   "source": [
    "args.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8097309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.step, step.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "90702fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000"
     ]
    }
   ],
   "source": [
    "args.step #, step.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b29c45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "012cb27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a8243f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb17d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000, 0.1)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f8cfca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3a0e3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.01\n",
    "args.n_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "95a45e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bbe3d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80d3a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_DESC = 'Setting up BNAF.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "43d1290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9976028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__ == '1.9.0+cu102'\n",
    "! pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1e79f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ec862c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLAB ###\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages\n",
    "! apt-get install tree >/dev/null\n",
    "\n",
    "# Download dataset\n",
    "# ! ./get_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db0ae336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format of files in $ROOT/data\n",
    "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0bbe1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c06254ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*|wandb*|run_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d1462463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=1024):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of hits to force in each jet.\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        jets = [self.parse_img(dict(data)['X_jets'][i]) for i in self.channels]\n",
    "        jets = jets[0]            # Temporary Hack for Ecal\n",
    "        return jets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)\n",
    "        # print(out.shape)\n",
    "\n",
    "        if self.max_instances:\n",
    "            # Pad instances with fewer hits\n",
    "            if out.shape[0] <= self.max_instances:\n",
    "                pad_len = self.max_instances - out.shape[0]\n",
    "                out = F.pad(out, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "            # Cut off excess hits from other jets\n",
    "            else:\n",
    "                out = out[:self.max_instances,:]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Output Shape:', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ef063d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8d270842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f024357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "608067d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        # print(x.dtype)\n",
    "        # print(w.dtype)\n",
    "        # print(x)\n",
    "        # print(w)\n",
    "        x = x.type(torch.float32)\n",
    "        # print(x.dtype)\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "95b0d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3b0330ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for idx in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.__getitem__(idx)\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item(), 'Learning_Rate': args.lr})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "51ea34c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 100, 0.01)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c48362b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.01\n",
    "args.n_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de823c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 100, 0.01)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bdd1edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69cee8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "139f0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8a5750c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2b597475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fe25082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "26bc1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c79cb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6bfd36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, to_sort=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if to_sort:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, to_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e2f7d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7255b0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e0cd0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ab055176",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "93749ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_prediction(data, pred, to_sort=False):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=[12,12])\n",
    "    labels = ['X', 'Y']\n",
    "    n_hits = len(data.nonzero()) // 2\n",
    "    data = data[:n_hits]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        pred_idx = np.array(range(0,1024))\n",
    "        data_idx = (1024 / n_hits) * np.array(range(0,n_hits))\n",
    "\n",
    "        if to_sort:\n",
    "            ax.plot(pred_idx, sorted(pred[:,i]), label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, sorted(data[:,i]), label='labels', c='navy')\n",
    "        else:\n",
    "            ax.plot(pred_idx, pred[:,i], label='prediction', c='lightskyblue')\n",
    "            ax.plot(data_idx, data[:,i], label='labels', c='navy')\n",
    "        \n",
    "        ax.set_title('Distribution of: {}'.format(labels[i]))\n",
    "        ax.grid()\n",
    "        ax.set_xlim(0,1025)\n",
    "        ax.set_xticks(range(0,1025,128))\n",
    "        ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "analyse_prediction(input_data, pred)\n",
    "analyse_prediction(input_data, pred, to_sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "02b62822",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.01\n",
    "args.n_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "df359ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 300, 0.01)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd1583ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8fae5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "args.n_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e011d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "args.n_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6a3b843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 500, 0.001)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "47793e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "03c57d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 500, 0.001)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d92986f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=0, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "629df6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "10ba12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ae9a86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0934cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=2, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4c0033c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5c4ea7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "20e14978",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7b000c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def eval_bnaf(model, dataset, instance_idx, scale=True, verbose=True):\n",
    "    input_data = dataset.__getitem__(instance_idx)\n",
    "    zzk, logdets = model(input_data.reshape(1,-1))\n",
    "    log_prob = model.base_dist.log_prob(zzk) + logdets\n",
    "    prob = log_prob.sum(1).exp().cpu()\n",
    "    pred = log_prob.reshape(1024,2).cpu().detach()\n",
    "\n",
    "    if verbose:\n",
    "        print('zzk:{}, logdets:{}'.format(zzk.shape, logdets.shape))\n",
    "        print('zzk:{}, logdets:{}'.format(prob.shape, log_prob.shape))\n",
    "        print('pred:{}'.format(pred.shape))\n",
    "\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        pred = scaler.fit_transform(pred) * 125\n",
    "        pred = torch.Tensor(pred)\n",
    "\n",
    "    return input_data, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "62d8ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "\n",
    "input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "vis(input_data)\n",
    "vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "91264e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(2:10):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data)\n",
    "    vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1bff2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(2,10):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data)\n",
    "    vis(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8c3d447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(2,3):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data, title='Data_Idx={:03d}'.format(IDX))\n",
    "    vis(pred, title='Pred_Idx={:03d}'.format(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fbd2bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(2,5):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data, title='Data_Idx={:03d}'.format(IDX))\n",
    "    vis(pred, title='Pred_Idx={:03d}'.format(IDX))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "03c08769",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(2,5):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data, title='Data_Idx={:03d}'.format(IDX))\n",
    "    vis(pred, title='Pred_Idx={:03d}'.format(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6be81332",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(5.15):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data, title='Data_Idx={:03d}'.format(IDX))\n",
    "    vis(pred, title='Pred_Idx={:03d}'.format(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bb567503",
   "metadata": {},
   "outputs": [],
   "source": [
    "for IDX in range(5,15):\n",
    "    input_data, pred = eval_bnaf(model, dataset, instance_idx=IDX, scale=True, verbose=False)\n",
    "    vis(input_data, title='Data_Idx={:03d}'.format(IDX))\n",
    "    vis(pred, title='Pred_Idx={:03d}'.format(IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3b60a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 500, 0.001)"
     ]
    }
   ],
   "source": [
    "args.step, args.n_steps, args.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "418dea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "args.n_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "30191b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a58f85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "args.n_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "675e8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "50e56d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1a175d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    print(z)\n",
    "    print(logdet)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a21b4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e6d08d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0dee4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5b704126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])"
     ]
    }
   ],
   "source": [
    "d = [1,2,3,4,5]\n",
    "d = torch.Tensor(d)\n",
    "d = d.type(torch.int)\n",
    "d = d.type(torch.float32)\n",
    "d\n",
    "# d.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d6526560",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        # print(x.dtype)\n",
    "        # print(w.dtype)\n",
    "        # print(x)\n",
    "        # print(w)\n",
    "        x = x.type(torch.float32)\n",
    "        # print(x.dtype)\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "acc6fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    print(z)\n",
    "    print(logdet)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a1a535e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for idx in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.__getitem__(idx)\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item(), 'Learning_Rate': args.lr})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "732bb64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8f7ee6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "10 in range(10, 20+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c8c846ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "10 in range(11, 20+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "64128237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "20 in range(11, 20+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "63b9e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "21 in range(11, 20+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b46ffbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "while i<20:\n",
    "    print(i)\n",
    "    i++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2257d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "while i<20:\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0a5928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        assert self.channels == [1]         # Temporary Hack for Ecal\n",
    "        self.cur_idx = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Use `get_next_valid_instance` instead`.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len // 3            # Set a safe limit.\n",
    "\n",
    "    def get_next_valid_instance(self):\n",
    "        allowed_range = range(self.min_instances, self.max_instances+1)\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jet = self.get_raw_instance(idx)\n",
    "                parsed_jet = self.parse_img(raw_jet)\n",
    "                if parsed_jet.shape[0] in allowed_range:\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('Skipped instance at idx={}, shape={}'.format(idx, parsed_jet.shape))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][1]\n",
    "        return raw_jet\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Output Shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2f5dbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "40e39527",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances=None)\n",
    "dataset.verbose = True\n",
    "print('Length of Dataset: ', dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ddcb759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances=None)\n",
    "dataset.verbose = True\n",
    "print('Length of Dataset: ', dataset.__len__())\n",
    "print('Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ae5f7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances=None)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "03456b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0486d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.get_next_valid_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3d3901f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e1a865a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1b328e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.get_next_valid_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a51dc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.get_next_valid_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "130d7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.get_next_valid_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bd3e019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        assert self.channels == [1]         # Temporary Hack for Ecal\n",
    "        self.cur_idx = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Use `get_next_valid_instance` instead`.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len // 3            # Set a safe limit.\n",
    "\n",
    "    def get_next_valid_instance(self):\n",
    "        allowed_range = range(self.min_instances, self.max_instances+1)\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jet = self.get_raw_instance(idx)\n",
    "                parsed_jet = self.parse_img(raw_jet)\n",
    "                if parsed_jet.shape[0] in allowed_range:\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    self.cur_idx = idx + 1\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('Skipped instance at idx={}, shape={}'.format(idx, parsed_jet.shape))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][1]\n",
    "        return raw_jet\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Output Shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "73678244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "15cf627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "069a415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        assert self.channels == [1]         # Temporary Hack for Ecal\n",
    "        self.cur_idx = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Use `get_next_valid_instance` instead`.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len // 3            # Set a safe limit.\n",
    "\n",
    "    def get_next_valid_instance(self):\n",
    "        '''\n",
    "        Returns a valid sample, with it's true index.\n",
    "        '''\n",
    "        allowed_range = range(self.min_instances, self.max_instances+1)\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jet = self.get_raw_instance(idx)\n",
    "                parsed_jet = self.parse_img(raw_jet)\n",
    "                if parsed_jet.shape[0] in allowed_range:\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    self.cur_idx = idx + 1\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('-- Skipped instance at idx={}, shape={}'.format(idx, parsed_jet.shape))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][1]\n",
    "        return raw_jet\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)        \n",
    "        if self.verbose:\n",
    "            print('-- Output Shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e84726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2f15c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65505619",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b454f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e34da76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e422ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    pirnt(dataset.cur_idx)\n",
    "    print('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3cee8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "83c29cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print(dataset.cur_idx)\n",
    "    print('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a0e194f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print(dataset.cur_idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3fba11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print(dataset.cur_idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9f2b54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.verbose = False\n",
    "        self.channels = channels  # Channels to process - currently only Ecal. \n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        assert self.channels == [1]         # Temporary Hack for Ecal\n",
    "        self.cur_idx = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Use `get_next_valid_instance` instead`.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return -1\n",
    "        return self.total_len // 3            # Set a safe limit.\n",
    "\n",
    "    def get_next_valid_instance(self):\n",
    "        '''\n",
    "        Returns a valid sample, with it's true index.\n",
    "        '''\n",
    "        allowed_range = range(self.min_instances, self.max_instances+1)\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jet = self.get_raw_instance(idx)\n",
    "                parsed_jet = self.parse_img(raw_jet)\n",
    "                if parsed_jet.shape[0] in allowed_range:\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    self.cur_idx = idx + 1\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('-- Skipped instance at idx={}, shape={}'.format(idx, parsed_jet.shape))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][1]\n",
    "        return raw_jet\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        Else returns an array of shape [N, 3] where N is the number of non-zero hits in each jet.\n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)        \n",
    "        if self.verbose:\n",
    "            print('-- Output Shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "571d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e41e2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1a6cfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_valid_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print(dataset.cur_idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "cf5a4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cur_idx = 0\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        self.cols = None \n",
    "        self.verbose = False                # False by default\n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Not needed. Using `get_next_valid_instance` instead.')\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError('Not needed. Using `get_next_valid_instance` instead.')\n",
    "\n",
    "    def get_next_instance(self):\n",
    "        '''\n",
    "        Returns the next valid sample, with it's true index.\n",
    "        Keeps looping until a valid sample is found.\n",
    "        '''\n",
    "        allowed_range = range(self.min_instances, self.max_instances+1)\n",
    "        \n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jet = self.get_raw_instance(idx)\n",
    "                parsed_jet = self.parse_img(raw_jet)\n",
    "                if parsed_jet.shape[0] in allowed_range:\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    self.cur_idx = idx + 1\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('-- Skipped instance at idx={}, shape={}'.format(idx, parsed_jet.shape))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][1]               # Temporary Hack for Ecal\n",
    "        return raw_jet\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos),dim=1)        \n",
    "        if self.verbose:\n",
    "            print('-- Output Shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7b27d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3d7eb69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Assumed Length of Dataset: ', dataset.__len__())\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3fc277c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "414f615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('[{}]'.format(i))\n",
    "    data_sample, true_idx = dataset.get_next_instance()\n",
    "    print(data_sample.shape, true_idx)\n",
    "    print(dataset.cur_idx)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4182947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a7571e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "43c19132",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        # print(x.dtype)\n",
    "        # print(w.dtype)\n",
    "        # print(x)\n",
    "        # print(w)\n",
    "        x = x.type(torch.float32)\n",
    "        # print(x.dtype)\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "04ef3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    print(z)\n",
    "    print(logdet)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "394db5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for idx in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.__getitem__(idx)\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item(), 'Learning_Rate': args.lr})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f3d336a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c4c7745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d4d9d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048*4   #3072\n",
    "args.n_steps = 500*2\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "09730fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.get_next_instance()\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item(), 'Learning_Rate': args.lr})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2960ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "89df2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048*4   #3072\n",
    "args.n_steps = 500*2\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "188e4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048*4   #3072\n",
    "args.n_steps = 10\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "84b78948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_123952-174v4wvp/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_123952-174v4wvp/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c88d15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "89348ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data, true_index = dataset.get_next_instance()\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, \n",
    "                   'Loss': loss.item(), \n",
    "                   'Learning_Rate': args.lr,\n",
    "                   'True_Index': true_index})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ef5ead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6e362326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 2048\n",
    "args.hidden_dim = 2048*4   #3072\n",
    "args.n_steps = 10\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ecbafb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_124527-bqzeo3n7/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_124527-bqzeo3n7/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "208e8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5805954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetmax_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "bc46ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.max_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7b459f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768"
     ]
    }
   ],
   "source": [
    "dataset.max_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9b2950de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 10\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e542de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_124909-1hcbbgjd/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210702_124909-1hcbbgjd/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "751ea363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "b94a53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    # print(z)\n",
    "    # print(logdet)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e8cdc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data, true_index = dataset.get_next_instance()\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, \n",
    "                   'Loss': loss.item(), \n",
    "                   'Learning_Rate': args.lr,\n",
    "                   'True_Index': true_index})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "88e9e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35846ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 1000\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9a8359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
