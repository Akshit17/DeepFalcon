{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ab6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_DESC = 'Setting up BNAF.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe2f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc43fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__ == '1.9.0+cu102'\n",
    "! pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c148341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cd9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf04c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLAB ###\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages\n",
    "! apt-get install tree >/dev/null\n",
    "\n",
    "# Download dataset\n",
    "# ! ./get_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a95ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format of files in $ROOT/data\n",
    "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f47c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DOING IMPORTS '''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabf3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d6ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "# ! tree -I 'model*|temp__*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ba36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' FUNCTIONS: DATA/INSTANCE LOADING '''\n",
    "\n",
    "# def load_dataset(dataset_file):\n",
    "#     f_path = DATA_ROOT + dataset_file\n",
    "#     data = pq.read_table(f_path)\n",
    "#     return data\n",
    "        \n",
    "# def get_instance(data, idx):\n",
    "#     assert idx < 32000\n",
    "#     instance = np.array(data[0][idx].as_py())\n",
    "#     return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3776787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' FUNCTIONS: PARSING/VISUALIZING IMAGES '''\n",
    "\n",
    "# def parse_img(track_img, reduce=False):\n",
    "#     if reduce:\n",
    "#         track_img = cv2.resize(track_img, dsize=(25,25))\n",
    "#         # plt.imshow(track_img)\n",
    "        \n",
    "#     x_pos = []\n",
    "#     y_pos = []\n",
    "#     val = []\n",
    "#     for x_idx in range(track_img.shape[0]):\n",
    "#         for y_idx in range(track_img.shape[1]):\n",
    "#             if track_img[x_idx][y_idx] != 0:\n",
    "#                 val.append(track_img[x_idx][y_idx])\n",
    "#                 x_pos.append(x_idx)\n",
    "#                 y_pos.append(y_idx)\n",
    "    \n",
    "#     if reduce:\n",
    "#         x_pos = [5*v for v in x_pos]\n",
    "#         y_pos = [5*v for v in y_pos]\n",
    "#     # print(len(x_pos), len(y_pos), len(val))\n",
    "#     return x_pos, y_pos, val\n",
    "\n",
    "# def vis(img, title=None, scale=1000, cmap='gist_heat', reduce=False):        \n",
    "#     x_pos, y_pos, val = parse_img(img, reduce)\n",
    "#     if scale:\n",
    "#         sz = np.array(np.abs(val))*scale\n",
    "#     else:\n",
    "#         sz = np.ones_like(val) * 10\n",
    "        \n",
    "#     plt.figure(figsize=[14,6], facecolor='#f0f0f0')\n",
    "#     cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "#     sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "#     plt.colorbar(sc)\n",
    "#     plt.xlim(0, 125)\n",
    "#     plt.ylim(0, 125)\n",
    "#     plt.grid()\n",
    "#     if title:\n",
    "#         plt.title(title)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa04c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' ANALYSING PARSED IMAGES '''\n",
    "\n",
    "# if DO_ANALYSIS:\n",
    "#     dataset_file = INPUT_FORMAT.format(0)\n",
    "#     data = load_dataset(dataset_file)\n",
    "    \n",
    "#     img = get_instance(data, 2)\n",
    "#     vis(img[0], scale=500, title='Track')\n",
    "#     vis(img[1], scale=200, title='ECAL')\n",
    "#     vis(img[2], scale=300, title='HCAL', reduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c65b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' LOADING THE SINGLE PARSED DATASET '''\n",
    "\n",
    "# def load_cached_dataset(file_name):\n",
    "#     with open(CACHE_ROOT + file_name, 'rb') as f:\n",
    "#         X_dict = pkl.load(f)\n",
    "#     X_track = X_dict['X_track']\n",
    "#     X_ECAL = X_dict['X_ECAL']\n",
    "#     X_HCAL = X_dict['X_HCAL']\n",
    "#     return X_track, X_ECAL, X_HCAL\n",
    "\n",
    "# X_track, X_ECAL, X_HCAL = load_cached_dataset('X_dict.pkl')\n",
    "# print(len(X_track), len(X_ECAL), len(X_HCAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_file = INPUT_FORMAT.format(0)\n",
    "# data = load_dataset(dataset_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "749f599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=1024):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cols = None \n",
    "        self.channels = channels\n",
    "        self.max_instances = max_instances\n",
    "        self.verbose = False\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        jets = [self.parse_img(dict(data)['X_jets'][i]) for i in self.channels]\n",
    "        jets = jets[0]            # Temporary Hack for ECal\n",
    "        return jets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.parquet.num_row_groups\n",
    "\n",
    "    def parse_img(self, track_img, reduce=False):\n",
    "        '''\n",
    "        Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "        If `max_instances` is defined then returns an array of shape [max_instances, 3].   \n",
    "        '''\n",
    "        track_img = torch.Tensor(track_img)        \n",
    "        x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "        val = track_img[x_pos, y_pos]\n",
    "        out = torch.stack((x_pos,y_pos,val),dim=1)\n",
    "        \n",
    "        if self.max_instances:\n",
    "            if out.shape[0] <= self.max_instances:\n",
    "                pad_len = self.max_instances - out.shape[0]\n",
    "                out = F.pad(out, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "            else:\n",
    "                out = out[:self.max_instances,:]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Output Shape:', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "626b6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):     \n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "    x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 30\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7687b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file, max_instances=None)\n",
    "# dataset.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1994ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928b51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sample = dataset.__getitem__(1000)\n",
    "# vis(data_sample, scale=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad9fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 1.7000e+01, 1.2246e-02])"
     ]
    }
   ],
   "source": [
    "data_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102a86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e2523a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9ee9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e286824",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    sample = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(sample)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb93dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    for idx in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data = dataset.__getitem__(idx)\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, 'Loss': loss.item()})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd20da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dca6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 3072\n",
    "args.hidden_dim = 6144   #3072\n",
    "args.n_steps = 500\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cbb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210625_124410-1437anuo/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210625_124410-1437anuo/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save \n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1f0af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = 3072\n",
    "args.hidden_dim = 6144   #3072\n",
    "args.n_steps = 500\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.3\n",
    "args.step = 0\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142fba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save \n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
