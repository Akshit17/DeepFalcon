{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45417f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_DESC = 'Setting up BNAF.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d6e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6432c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__ == '1.9.0+cu102'\n",
    "! pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcf0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711457b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLAB ###\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages\n",
    "! apt-get install tree >/dev/null\n",
    "\n",
    "# Download dataset\n",
    "# ! ./get_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27c114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLAB ###\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install missing packages\n",
    "! apt-get install tree >/dev/null\n",
    "\n",
    "# Download dataset\n",
    "# ! ./get_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978de587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format of files in $ROOT/data\n",
    "INPUT_FORMAT = 'Boosted_Jets_Sample-{}.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38355c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set Numpy Print Options\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5c0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE AND SET DATA/CACHE DIRECTORIES '''\n",
    "\n",
    "ROOT = '/content/drive/My Drive/_GSoC/Normalizing-Flows/'\n",
    "DATA_ROOT = ROOT + \"data/\"\n",
    "CACHE_ROOT = ROOT + \"cache/\"\n",
    "LOGS_ROOT = ROOT + \"logs/\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CACHE_ROOT, exist_ok=True)\n",
    "os.makedirs(LOGS_ROOT, exist_ok=True)\n",
    "\n",
    "# Initialize scratch space on /content for faster read-write\n",
    "SCRATCH_ROOT = '/content/scratch/'   \n",
    "os.makedirs(SCRATCH_ROOT, exist_ok=True) \n",
    "\n",
    "# print('Directory Structure [Excluding Log/Temp Files]:')\n",
    "! tree -I 'model*|temp__*|wandb*|run_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a9f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import *\n",
    "CHANNELS = ['Pt', 'ECal', 'HCal']\n",
    "\n",
    "def parse_img(track_img, reduce=False):\n",
    "    '''\n",
    "    Returns non-zero hits from the single-channel input jet image - `track_img`. \n",
    "    '''\n",
    "    track_img = torch.Tensor(track_img)        \n",
    "    x_pos, y_pos = torch.nonzero(track_img, as_tuple=True)\n",
    "    val = track_img[x_pos, y_pos]\n",
    "    out = torch.stack((x_pos,y_pos,val),dim=1)\n",
    "    return out\n",
    "\n",
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, filename, channels=[1], max_instances=768, min_instances=384):\n",
    "        self.parquet = pq.ParquetFile(filename)\n",
    "        self.cur_idx = 0\n",
    "        self.total_len = self.parquet.num_row_groups\n",
    "        self.cols = None \n",
    "        self.verbose = False                # False by default\n",
    "        self.max_instances = max_instances  # Number of max hits to force in each jet.\n",
    "        self.min_instances = min_instances  # Number of min hits to force in each jet.\n",
    "        self.allowed_range = range(min_instances, max_instances+1)\n",
    "\n",
    "        self.pt_range = [25,140]\n",
    "        self.return_channels = ['ECal']\n",
    "        self.supress_val = True\n",
    "        self.cached_pts = None\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError('Not needed. Using `get_next_valid_instance` instead.')\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError('Not needed. Using `get_next_valid_instance` instead.')\n",
    "\n",
    "    def get_next_instance(self):\n",
    "        '''\n",
    "        Returns the next valid sample, with it's true index.\n",
    "        Keeps looping until a valid sample is found.\n",
    "        '''\n",
    "\n",
    "        while True:\n",
    "            for idx in range(self.cur_idx, self.total_len):\n",
    "                raw_jets = self.get_raw_instance(idx)\n",
    "                parsed_jets = [parse_img(j) for j in raw_jets]\n",
    "                gen_pts = [j[:,2] for j in parsed_jets]\n",
    "                self.cached_pts = gen_pts\n",
    "                \n",
    "                if self.supress_val:\n",
    "                    parsed_jets = [j[:,:2] for j in parsed_jets]\n",
    "                \n",
    "                ### Temporary hack for ECal ###\n",
    "                ecal_idx = self.return_channels.index('ECal')\n",
    "                parsed_jet = parsed_jets[ecal_idx]      \n",
    "                gen_pt = gen_pts[ecal_idx].sum() \n",
    "                if self.verbose:\n",
    "                    print('-- Output Shape: {}'.format(parsed_jet.shape))\n",
    "                    print('-- Gen-Pt Val: {}'.format(gen_pt))\n",
    "\n",
    "                # print(gen_pt, self.pt_range)\n",
    "                if (parsed_jet.shape[0] in self.allowed_range) and \\\n",
    "                   (gen_pt >= self.pt_range[0] and gen_pt <= self.pt_range[1]):\n",
    "                    if self.verbose:\n",
    "                        print('-- Returning instance at idx={}'.format(idx))\n",
    "\n",
    "                    padded_jet = self.pad_instance(parsed_jet)\n",
    "                    self.cur_idx = idx + 1\n",
    "                    return padded_jet, idx     # Exit after finding a valid instance\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print('-- Skipped instance at idx={}, shape={}, pt={}'.format(idx, parsed_jet.shape, gen_pt))\n",
    "\n",
    "            # End of dataset, loop back.\n",
    "            self.cur_idx = 0\n",
    "\n",
    "    def get_gen_pt(self, raw_jets):\n",
    "        '''\n",
    "        Does not actually returns gen-pt, but an approximation equal to \n",
    "        the sum of ECal energies over all the hits.\n",
    "        '''\n",
    "        \n",
    "        return gen_pts\n",
    "\n",
    "    def get_raw_instance(self, index):\n",
    "\n",
    "        c_idx = []\n",
    "        for c in self.return_channels:\n",
    "            assert c in CHANNELS\n",
    "            c_idx.append(CHANNELS.index(c))\n",
    "\n",
    "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
    "        data['X_jets'] = np.float32(data['X_jets'][0]) \n",
    "        data['X_jets'] = data['X_jets'][0:]\n",
    "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.     # Zero-Suppression\n",
    "        raw_jet = dict(data)['X_jets'][c_idx]           # Temporary Hack for Ecal\n",
    "        return raw_jet\n",
    "\n",
    "    def pad_instance(self, instance):\n",
    "        assert instance.shape[0] <= self.max_instances\n",
    "        assert instance.shape[0] >= self.min_instances\n",
    "        pad_len = self.max_instances - instance.shape[0]\n",
    "        instance = F.pad(instance, pad=(0, 0, 0, pad_len), mode='constant', value=0)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160a5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(arr, is_parsed=True, title=None, scale=1000, cmap='gist_heat', reduce=False):  \n",
    "    '''\n",
    "    Visualise a jet instance.\n",
    "    '''\n",
    "\n",
    "    if not is_parsed:   \n",
    "        arr = parse_img(arr, reduce)\n",
    "\n",
    "    if arr.shape[1] == 3:\n",
    "        x_pos, y_pos, val = arr[:,0], arr[:,1], arr[:,2]\n",
    "    elif arr.shape[1] == 2:\n",
    "        x_pos, y_pos = arr[:,0], arr[:,1]\n",
    "        val = torch.ones_like(x_pos)\n",
    "        scale = None\n",
    "\n",
    "    if scale:\n",
    "        sz = np.array(np.abs(val)) * scale\n",
    "    else:\n",
    "        sz = np.ones_like(val) * 10\n",
    "        \n",
    "    plt.figure(figsize=[10,6], facecolor='#f0f0f0')\n",
    "    cm = plt.cm.get_cmap(cmap)     # 'gist_heat' / 'YlOrRd'\n",
    "    sc = plt.scatter(x_pos, y_pos, c=val, s=sz, cmap=cm, alpha=0.5, edgecolors='k')\n",
    "    plt.colorbar(sc)\n",
    "    plt.xlim(0, 126)\n",
    "    plt.ylim(0, 126)\n",
    "    plt.xticks(range(0,125,25))\n",
    "    plt.yticks(range(0,125,25))\n",
    "    plt.grid()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d4b3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.verbose = True\n",
    "print('Max Length of Dataset: ', dataset.total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9596e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample, true_idx = dataset.get_next_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d7241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 2])"
     ]
    }
   ],
   "source": [
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f9a69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.allowed_range)\n",
    "# print()\n",
    "\n",
    "# for i in range(10):\n",
    "#     print('[{}]'.format(i))\n",
    "#     data_sample, true_idx = dataset.get_next_instance()\n",
    "#     print(data_sample.shape, true_idx)\n",
    "#     print(dataset.cur_idx)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e4ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08c740fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', action='store_true', help='Train a flow.')\n",
    "    parser.add_argument('--plot', action='store_false', help='Plot a flow and target density.')\n",
    "    parser.add_argument('--restore_file', type=str, help='Path to model to restore.')\n",
    "    parser.add_argument('--output_dir', default='./results/run_')\n",
    "    parser.add_argument('--cuda', type=int, help='Which GPU to run on.')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='Random seed.')\n",
    "    # model parameters\n",
    "    parser.add_argument('--data_dim', type=int, default=2, help='Dimension of the data.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=100, help='Dimensions of hidden layers.')\n",
    "    parser.add_argument('--n_hidden', type=int, default=3, help='Number of hidden layers.')\n",
    "    # training parameters\n",
    "    parser.add_argument('--step', type=int, default=0, help='Current step of training (number of minibatches processed).')\n",
    "    parser.add_argument('--n_steps', type=int, default=1, help='Number of steps to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-1, help='Initial learning rate.')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='Learning rate decay.')\n",
    "    parser.add_argument('--lr_patience', type=float, default=2000, help='Number of steps before decaying learning rate.')\n",
    "    parser.add_argument('--log_interval', type=int, default=50, help='How often to save model and samples.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir = os.path.join('./results/run_', time.strftime('%Y-%m-%d_%H-%M-%S', time.gmtime()))\n",
    "    if not os.path.isdir(args.output_dir): \n",
    "        os.makedirs(args.output_dir)\n",
    "    args.device = torch.device('cuda:{}'.format(args.cuda) if args.cuda is not None and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068aace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model components ''' \n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, data_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.data_dim = data_dim\n",
    "        # print(self.in_features, self.out_features, self.data_dim)\n",
    "\n",
    "        # Notation:\n",
    "        # BNAF weight calculation for (eq 8): W = g(W) * M_d + W * M_o\n",
    "        #   where W is block lower triangular so model is autoregressive,\n",
    "        #         g = exp function; M_d is block diagonal mask; M_o is block off-diagonal mask.\n",
    "        # Weight Normalization (Salimans & Kingma, eq 2): w = g * v / ||v||\n",
    "        #   where g is scalar, v is k-dim vector, ||v|| is Euclidean norm\n",
    "        # ------\n",
    "        # Here: pre-weight norm matrix is v; then: v = exp(weight) * mask_d + weight * mask_o\n",
    "        #       weight-norm scalar is g: out_features dimensional vector (here logg is used instead to avoid taking logs in the logdet calc.\n",
    "        #       then weight-normed weight matrix is w = g * v / ||v||\n",
    "        #\n",
    "        #       log det jacobian of block lower triangular is taking block diagonal mask of\n",
    "        #           log(g*v/||v||) = log(g) + log(v) - log(||v||)\n",
    "        #                          = log(g) + weight - log(||v||) since v = exp(weight) * mask_d + weight * mask_o\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        mask_d = torch.zeros_like(weight)\n",
    "        mask_o = torch.zeros_like(weight)\n",
    "        for i in range(data_dim):\n",
    "            # select block slices\n",
    "            h     = slice(i * out_features // data_dim, (i+1) * out_features // data_dim)\n",
    "            w     = slice(i * in_features // data_dim,  (i+1) * in_features // data_dim)\n",
    "            w_row = slice(0,                            (i+1) * in_features // data_dim)\n",
    "            # initialize block-lower-triangular weight and construct block diagonal mask_d and lower triangular mask_o\n",
    "            nn.init.kaiming_uniform_(weight[h,w_row], a=math.sqrt(5))  # default nn.Linear weight init only block-wise\n",
    "            mask_d[h,w] = 1\n",
    "            mask_o[h,w_row] = 1\n",
    "\n",
    "        mask_o = mask_o - mask_d  # remove diagonal so mask_o is lower triangular 1-off the diagonal\n",
    "\n",
    "        self.weight = nn.Parameter(weight)                          # pre-mask, pre-weight-norm\n",
    "        self.logg = nn.Parameter(torch.rand(out_features, 1).log()) # weight-norm parameter\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.rand(out_features), -1/math.sqrt(in_features), 1/math.sqrt(in_features)))  # default nn.Linear bias init\n",
    "        self.register_buffer('mask_d', mask_d)\n",
    "        self.register_buffer('mask_o', mask_o)\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # 1. compute BNAF masked weight eq 8\n",
    "        v = self.weight.exp() * self.mask_d + self.weight * self.mask_o\n",
    "        # 2. weight normalization\n",
    "        v_norm = v.norm(p=2, dim=1, keepdim=True)\n",
    "        w = self.logg.exp() * v / v_norm\n",
    "        # 3. compute output and logdet of the layer\n",
    "        # print(x.dtype)\n",
    "        # print(w.dtype)\n",
    "        # print(x)\n",
    "        # print(w)\n",
    "        x = x.type(torch.float32)\n",
    "        # print(x.dtype)\n",
    "        out = F.linear(x, w, self.bias)\n",
    "        # print('Out:', out.shape)\n",
    "\n",
    "\n",
    "        logdet = self.logg + self.weight - 0.5 * v_norm.pow(2).log()\n",
    "        logdet = logdet[self.mask_d.byte()]\n",
    "        logdet = logdet.view(1, self.data_dim, out.shape[1]//self.data_dim, x.shape[1]//self.data_dim) \\\n",
    "                       .expand(x.shape[0],-1,-1,-1)  # output (B, data_dim, out_dim // data_dim, in_dim // data_dim)\n",
    "\n",
    "        # 4. sum with sum_logdets from layers before (BNAF section 3.3)\n",
    "        # Compute log det jacobian of the flow (eq 9, 10, 11) using log-matrix multiplication of the different layers.\n",
    "        # Specifically for two successive MaskedLinear layers A -> B with logdets A and B of shapes\n",
    "        #  logdet A is (B, data_dim, outA_dim, inA_dim)\n",
    "        #  logdet B is (B, data_dim, outB_dim, inB_dim) where outA_dim = inB_dim\n",
    "        #\n",
    "        #  Note -- in the first layer, inA_dim = in_features//data_dim = 1 since in_features == data_dim.\n",
    "        #            thus logdet A is (B, data_dim, outA_dim, 1)\n",
    "        #\n",
    "        #  Then:\n",
    "        #  logsumexp(A.transpose(2,3) + B) = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, inB_dim) , dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, 1, outA_dim) + (B, data_dim, outB_dim, outA_dim), dim=-1)\n",
    "        #                                  = logsumexp( (B, data_dim, outB_dim, outA_dim), dim=-1) where dim2 of tensor1 is broadcasted\n",
    "        #                                  = (B, data_dim, outB_dim, 1)\n",
    "\n",
    "        sum_logdets = torch.logsumexp(sum_logdets.transpose(2,3) + logdet, dim=-1, keepdim=True)\n",
    "\n",
    "        return out, sum_logdets\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, sum_logdets):\n",
    "        # derivation of logdet:\n",
    "        # d/dx tanh = 1 / cosh^2; cosh = (1 + exp(-2x)) / (2*exp(-x))\n",
    "        # log d/dx tanh = - 2 * log cosh = -2 * (x - log 2 + log(1 + exp(-2x)))\n",
    "        logdet = -2 * (x - math.log(2) + F.softplus(-2*x))\n",
    "        sum_logdets = sum_logdets + logdet.view_as(sum_logdets)\n",
    "        return x.tanh(), sum_logdets\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
    "    def forward(self, x):\n",
    "        sum_logdets = torch.zeros(1, x.shape[1], 1, 1, device=x.device)\n",
    "        for module in self:\n",
    "            x, sum_logdets = module(x, sum_logdets)\n",
    "        return x, sum_logdets.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8794fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "\n",
    "class BNAF(nn.Module):\n",
    "    def __init__(self, data_dim, n_hidden, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution for calculation of log prob under the model\n",
    "        self.register_buffer('base_dist_mean', torch.zeros(data_dim))\n",
    "        self.register_buffer('base_dist_var', torch.ones(data_dim))\n",
    "\n",
    "        # construct model\n",
    "        modules = []\n",
    "        modules += [MaskedLinear(data_dim, hidden_dim, data_dim), Tanh()]\n",
    "        for _ in range(n_hidden):\n",
    "            modules += [MaskedLinear(hidden_dim, hidden_dim, data_dim), Tanh()]\n",
    "        modules += [MaskedLinear(hidden_dim, data_dim, data_dim)]\n",
    "        self.net = FlowSequential(*modules)\n",
    "\n",
    "        # TODO --   add permutation\n",
    "        #           add residual gate\n",
    "        #           add stack of flows\n",
    "\n",
    "    @property\n",
    "    def base_dist(self):\n",
    "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compute_kl_pq_loss(model, input_data):\n",
    "    input_data = input_data.to(model.base_dist.loc.device)\n",
    "    z, logdet = model(input_data)\n",
    "    # print(z)\n",
    "    # print(logdet)\n",
    "    return - torch.sum(model.base_dist.log_prob(z) + logdet, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78b653e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_data, true_index = dataset.get_next_instance()\n",
    "        input_data = input_data.reshape(1,-1)\n",
    "        loss = loss_fn(model, input_data).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, \n",
    "                   'Loss': loss.item(), \n",
    "                   'Learning_Rate': args.lr,\n",
    "                   'True_Index': true_index})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac38294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54db7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 1000\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "args.pt_range = [50,55]\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.pt_range = args.pt_range\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac935e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_095112-vgu4i3pi/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_095112-vgu4i3pi/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96024ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.array([10,5])\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4dfa787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([10,5])\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a87fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.ones([1,2,3,4,5,6,7,8,9]).rehsape(1,-1)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77e401ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 362880])"
     ]
    }
   ],
   "source": [
    "k = torch.ones([1,2,3,4,5,6,7,8,9]).reshape(1,-1)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "895ccbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([1,2,3,4,5,6,7,8,9]).reshape(1,-1)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a4ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 9])"
     ]
    }
   ],
   "source": [
    "torch.stack([k,k,k]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d81c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9])"
     ]
    }
   ],
   "source": [
    "k = torch.Tensor([1,2,3,4,5,6,7,8,9])#.reshape(1,-1)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9734d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])"
     ]
    }
   ],
   "source": [
    "torch.stack([k,k,k]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e6cb281",
   "metadata": {},
   "outputs": [],
   "source": [
    " input_data, true_index = dataset.get_next_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25568d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 2])"
     ]
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcb40df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768, 2])"
     ]
    }
   ],
   "source": [
    "torch.stack([input_data,input_data,input_data]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa18330",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = input_data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc44caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1536])"
     ]
    }
   ],
   "source": [
    "torch.stack([k,k,k]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46035394",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_batch = []\n",
    "        for batch_idx in args.batch_size:\n",
    "            input_data, true_index = dataset.get_next_instance()\n",
    "            input_data = input_data.reshape(-1)\n",
    "            input_batch.append(input_data)\n",
    "        input_batch = torch.stack(input_batch)\n",
    "        print(input_batch.shape)\n",
    "        loss = loss_fn(model, input_batch).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, \n",
    "                   'Loss': loss.item(), \n",
    "                   'Learning_Rate': args.lr,\n",
    "                   'True_Index': true_index})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a12970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49aeb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 1000\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "args.pt_range = [50,55]\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.pt_range = args.pt_range\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b775d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 1000\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "args.pt_range = [50,55]\n",
    "args.batch_size = 16\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.pt_range = args.pt_range\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92a47a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_100332-3gnvetlu/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_100332-3gnvetlu/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66f8bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_100551-36ppoz1d/files/requirements.txt',\n",
      " '/content/drive/My Drive/_GSoC/Normalizing-Flows/logs/wandb/run-20210714_100551-36ppoz1d/files/requirements.txt']"
     ]
    }
   ],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c976d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Train The Model\n",
    "train_flow(model, dataset, loss_fn, optimizer, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56052c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training ''' \n",
    "\n",
    "def train_flow(model, dataset, loss_fn, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    init_steps = args.step\n",
    "    for _ in range(args.n_steps):\n",
    "        args.step += 1\n",
    "\n",
    "        input_batch = []\n",
    "        for batch_idx in range(args.batch_size):\n",
    "            input_data, true_index = dataset.get_next_instance()\n",
    "            input_data = input_data.reshape(-1)\n",
    "            input_batch.append(input_data)\n",
    "        input_batch = torch.stack(input_batch)\n",
    "        print(input_batch.shape)\n",
    "        loss = loss_fn(model, input_batch).mean(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        print(\"Step: {:03d}/{:03d} | Loss: {:14.2f}\".format(args.step, init_steps+args.n_steps, loss.item()))\n",
    "        wandb.log({'Step': args.step, \n",
    "                   'Loss': loss.item(), \n",
    "                   'Learning_Rate': args.lr,\n",
    "                   'True_Index': true_index})\n",
    "\n",
    "        if args.step % args.log_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'step': args.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'checkpoint.pt'))\n",
    "            torch.save({'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'optim_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51df72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to initialize wandb.\n",
    "DEFAULT_CFG = {\n",
    "    'model': 'BNAF_Valid-Samples',\n",
    "    'root_dir': ROOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22bd7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get Args\n",
    "args = get_args()\n",
    "\n",
    "# Set custom args\n",
    "args.data_dim = dataset.max_instances * 2     # 768 * 2\n",
    "args.hidden_dim = dataset.max_instances * 4   # 768 * 4\n",
    "args.n_steps = 1000\n",
    "args.log_interval = 200\n",
    "args.cuda = 0\n",
    "args.lr = 0.1\n",
    "args.step = 0\n",
    "args.pt_range = [50,55]\n",
    "args.batch_size = 16\n",
    "\n",
    "# Set Seeds\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Get Model\n",
    "model = BNAF(args.data_dim, args.n_hidden, args.hidden_dim).to(args.device)\n",
    "if args.restore_file:\n",
    "    model_checkpoint = torch.load(args.restore_file, map_location=args.device)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    args.step = model_checkpoint['step']\n",
    "\n",
    "# Save Config\n",
    "config = 'Parsed args:\\n{}\\n\\n'.format(pprint.pformat(args.__dict__)) + \\\n",
    "            'Num trainable params: {:,.0f}\\n\\n'.format(sum(p.numel() for p in model.parameters())) + \\\n",
    "            'Model:\\n{}'.format(model)\n",
    "\n",
    "config_path = os.path.join(args.output_dir, 'config.txt')\n",
    "if not os.path.exists(config_path):\n",
    "    with open(config_path, 'a') as f:\n",
    "        print(config, file=f)\n",
    "\n",
    "# Get Optimizer + Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.lr_decay, patience=args.lr_patience, verbose=True)\n",
    "if args.restore_file:\n",
    "    optim_checkpoint = torch.load(os.path.dirname(args.restore_file) + '/optim_checkpoint.pt', map_location=args.device)\n",
    "    optimizer.load_state_dict(optim_checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(optim_checkpoint['scheduler'])\n",
    "\n",
    "# Initialize Dataset \n",
    "dataset_file = DATA_ROOT + INPUT_FORMAT.format(0)\n",
    "dataset = ParquetDataset(dataset_file)\n",
    "dataset.pt_range = args.pt_range\n",
    "\n",
    "# Define Loss\n",
    "loss_fn = compute_kl_pq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03924256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_NOTES\"] = WANDB_DESC\n",
    "run = wandb.init(project='gnf', config=DEFAULT_CFG, dir=LOGS_ROOT)\n",
    "wandb.config.update(args)\n",
    "\n",
    "# Save files for later\n",
    "! pip freeze > requirements.txt\n",
    "wandb.save(ROOT + 'requirements.txt')\n",
    "# wandb.save(config_path)       # TODO: Fix permission issue on Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
